{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50bf403-76eb-4e6d-ae58-7c8661be8588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV from: /home/mubasshir/Desktop/Research/Insurance/DataSet_Patient/Medicare_Physician_Data/CMSData_sampled.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167730/726286891.py:46: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shape: (292663, 29)\n",
      "Preserving ID columns (not encoded): []\n",
      "Numeric columns (9): ['Rndrng_NPI', 'Rndrng_Prvdr_RUCA', 'Tot_Benes', 'Tot_Srvcs', 'Tot_Bene_Day_Srvcs', 'Avg_Sbmtd_Chrg', 'Avg_Mdcr_Alowd_Amt', 'Avg_Mdcr_Pymt_Amt', 'Avg_Mdcr_Stdzd_Amt']\n",
      "Categorical columns (20): ['Rndrng_Prvdr_Last_Org_Name', 'Rndrng_Prvdr_First_Name', 'Rndrng_Prvdr_MI', 'Rndrng_Prvdr_Crdntls', 'Rndrng_Prvdr_Gndr', 'Rndrng_Prvdr_Ent_Cd', 'Rndrng_Prvdr_St1', 'Rndrng_Prvdr_St2', 'Rndrng_Prvdr_City', 'Rndrng_Prvdr_State_Abrvtn', 'Rndrng_Prvdr_State_FIPS', 'Rndrng_Prvdr_Zip5', 'Rndrng_Prvdr_RUCA_Desc', 'Rndrng_Prvdr_Cntry', 'Rndrng_Prvdr_Type', 'Rndrng_Prvdr_Mdcr_Prtcptg_Ind', 'HCPCS_Cd', 'HCPCS_Desc', 'HCPCS_Drug_Ind', 'Place_Of_Srvc']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 217\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 125\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m train_df, val_df \u001b[38;5;241m=\u001b[39m train_test_split(train_df, test_size\u001b[38;5;241m=\u001b[39mVAL_SIZE, random_state\u001b[38;5;241m=\u001b[39mRANDOM_SEED)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Build and fit preprocessor\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m preprocessor, num_cols, cat_cols \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_preprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting preprocessor on training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m preprocessor\u001b[38;5;241m.\u001b[39mfit(train_df)\n",
      "Cell \u001b[0;32mIn[3], line 67\u001b[0m, in \u001b[0;36mbuild_preprocessor\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategorical columns (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cat_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m numeric_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     61\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     62\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler())\n\u001b[1;32m     63\u001b[0m ])\n\u001b[1;32m     65\u001b[0m categorical_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     66\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__MISSING__\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m---> 67\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     68\u001b[0m ])\n\u001b[1;32m     70\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer([\n\u001b[1;32m     71\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m, numeric_pipeline, num_cols),\n\u001b[1;32m     72\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, categorical_pipeline, cat_cols)\n\u001b[1;32m     73\u001b[0m ], remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m, sparse_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preprocessor, num_cols, cat_cols\n",
      "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scikit-learn autoencoder for tabular data (no TensorFlow).\n",
    "Trains an MLPRegressor to reconstruct the input and extracts the central hidden layer as encoded features.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import joblib\n",
    "\n",
    "# -----------------------\n",
    "# USER CONFIG\n",
    "# -----------------------\n",
    "INPUT_CSV = \"/home/mubasshir/Desktop/Research/Insurance/DataSet_Patient/Medicare_Physician_Data/CMSData_sampled.csv\"  # <-- change to your CSV path\n",
    "OUTPUT_DIR = \"/\"\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "BATCH_SIZE = 256\n",
    "MAX_ITER = 400\n",
    "LATENT_DIM = 16\n",
    "HIDDEN_UNITS_ENCODER = [128, 64]  # encoder side; will mirror for decoder\n",
    "ACTIVATION = \"relu\"\n",
    "EARLY_STOPPING = True\n",
    "VERBOSE = True\n",
    "SAVE_TRANSFORMER = True\n",
    "SAVE_MODEL = True\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def read_csv(path: str) -> pd.DataFrame:\n",
    "    print(f\"Loading CSV from: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Loaded shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def detect_columns(df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    return numeric_cols, categorical_cols\n",
    "\n",
    "def build_preprocessor(df: pd.DataFrame):\n",
    "    num_cols, cat_cols = detect_columns(df)\n",
    "    print(f\"Numeric columns ({len(num_cols)}): {num_cols}\")\n",
    "    print(f\"Categorical columns ({len(cat_cols)}): {cat_cols}\")\n",
    "\n",
    "    numeric_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", numeric_pipeline, num_cols),\n",
    "        (\"cat\", categorical_pipeline, cat_cols)\n",
    "    ], remainder=\"drop\", sparse_threshold=0)\n",
    "\n",
    "    return preprocessor, num_cols, cat_cols\n",
    "\n",
    "def symmetric_hidden_layers(encoder_units: List[int], latent_dim: int) -> Tuple[Tuple[int,...], int]:\n",
    "    left = list(encoder_units)\n",
    "    right = list(reversed(encoder_units))\n",
    "    hidden = tuple(left + [latent_dim] + right)\n",
    "    latent_index = len(left)\n",
    "    return hidden, latent_index\n",
    "\n",
    "def forward_to_layer(X: np.ndarray, coefs: List[np.ndarray], intercepts: List[np.ndarray],\n",
    "                     layer_idx: int, activation: str = \"relu\") -> np.ndarray:\n",
    "    def act(x):\n",
    "        if activation == \"relu\":\n",
    "            return np.maximum(0, x)\n",
    "        elif activation == \"tanh\":\n",
    "            return np.tanh(x)\n",
    "        elif activation in (\"logistic\", \"sigmoid\"):\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "        else:\n",
    "            return x\n",
    "    a = X.copy()\n",
    "    for i in range(layer_idx + 1):\n",
    "        W = coefs[i]\n",
    "        b = intercepts[i]\n",
    "        a = a.dot(W) + b\n",
    "        a = act(a)\n",
    "    return a\n",
    "\n",
    "# -----------------------\n",
    "# Main pipeline\n",
    "# -----------------------\n",
    "def main():\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    df = read_csv(INPUT_CSV)\n",
    "    if df.shape[0] < 5:\n",
    "        raise SystemExit(\"Dataset too small to train an autoencoder.\")\n",
    "\n",
    "    # Detect and preserve probable ID columns (unique-per-row)\n",
    "    possible_id_cols = [c for c in df.columns if c.lower() in (\"id\", \"npi\", \"physician_id\", \"provider_id\") or c.lower().endswith(\"_id\")]\n",
    "    id_cols = [c for c in possible_id_cols if df[c].nunique() == df.shape[0]]\n",
    "    print(f\"Preserving ID columns (not encoded): {id_cols}\")\n",
    "\n",
    "    df_features = df.drop(columns=id_cols, errors=\"ignore\")\n",
    "\n",
    "    # Split into train/test and a small validation set\n",
    "    train_df, test_df = train_test_split(df_features, test_size=TEST_SIZE, random_state=RANDOM_SEED, shuffle=True)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=VAL_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "    # Build and fit preprocessor\n",
    "    preprocessor, num_cols, cat_cols = build_preprocessor(train_df)\n",
    "    print(\"Fitting preprocessor on training data...\")\n",
    "    preprocessor.fit(train_df)\n",
    "    X_train = preprocessor.transform(train_df)\n",
    "    X_val = preprocessor.transform(val_df)\n",
    "    X_test = preprocessor.transform(test_df)\n",
    "\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    X_val = np.asarray(X_val, dtype=np.float32)\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    print(f\"Transformed input dimension: {input_dim}\")\n",
    "\n",
    "    if SAVE_TRANSFORMER:\n",
    "        transformer_path = os.path.join(OUTPUT_DIR, \"preprocessor.joblib\")\n",
    "        joblib.dump(preprocessor, transformer_path)\n",
    "        print(f\"Saved preprocessor to {transformer_path}\")\n",
    "\n",
    "    hidden_layers, latent_layer_index = symmetric_hidden_layers(HIDDEN_UNITS_ENCODER, LATENT_DIM)\n",
    "    print(f\"Using hidden_layer_sizes (MLP): {hidden_layers} with latent at index {latent_layer_index}\")\n",
    "\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=hidden_layers,\n",
    "                       activation=ACTIVATION,\n",
    "                       solver=\"adam\",\n",
    "                       alpha=1e-5,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       learning_rate_init=1e-3,\n",
    "                       max_iter=MAX_ITER,\n",
    "                       early_stopping=EARLY_STOPPING,\n",
    "                       validation_fraction=0.15,\n",
    "                       n_iter_no_change=20,\n",
    "                       random_state=RANDOM_SEED,\n",
    "                       verbose=VERBOSE)\n",
    "\n",
    "    print(\"Training MLP autoencoder (X -> X)...\")\n",
    "    mlp.fit(X_train, X_train)\n",
    "\n",
    "    if SAVE_MODEL:\n",
    "        model_path = os.path.join(OUTPUT_DIR, \"mlp_autoencoder.joblib\")\n",
    "        joblib.dump(mlp, model_path)\n",
    "        print(f\"Saved MLP autoencoder to {model_path}\")\n",
    "\n",
    "    recon_test = mlp.predict(X_test)\n",
    "    mse_per_sample = np.mean((recon_test - X_test) ** 2, axis=1)\n",
    "    print(f\"Test reconstruction MSE: mean={mse_per_sample.mean():.6f}, std={mse_per_sample.std():.6f}\")\n",
    "\n",
    "    test_with_ids = test_df.reset_index(drop=True).copy()\n",
    "    if id_cols:\n",
    "        original_train, original_test = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_SEED, shuffle=True)\n",
    "        test_ids = original_test.reset_index(drop=True)[id_cols]\n",
    "        test_with_ids = pd.concat([test_ids.reset_index(drop=True), test_with_ids.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    test_with_ids[\"reconstruction_mse\"] = mse_per_sample\n",
    "    results_path = os.path.join(OUTPUT_DIR, \"test_reconstruction_errors.csv\")\n",
    "    test_with_ids.to_csv(results_path, index=False)\n",
    "    print(f\"Saved test reconstruction errors to: {results_path}\")\n",
    "\n",
    "    # Extract latent representations for entire dataset\n",
    "    all_transformed = preprocessor.transform(df_features)\n",
    "    all_transformed = np.asarray(all_transformed, dtype=np.float32)\n",
    "    coefs = mlp.coefs_\n",
    "    intercepts = mlp.intercepts_\n",
    "    encoded_all = forward_to_layer(all_transformed, coefs, intercepts, layer_idx=latent_layer_index, activation=ACTIVATION)\n",
    "\n",
    "    encoded_cols = [f\"enc_{i}\" for i in range(encoded_all.shape[1])]\n",
    "    encoded_df = pd.DataFrame(encoded_all, columns=encoded_cols)\n",
    "    if id_cols:\n",
    "        encoded_df = pd.concat([df[id_cols].reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    encoded_out_path = os.path.join(OUTPUT_DIR, \"encoded_features.csv\")\n",
    "    encoded_df.to_csv(encoded_out_path, index=False)\n",
    "    print(f\"Saved encoded features to: {encoded_out_path}\")\n",
    "\n",
    "    meta = {\n",
    "        \"input_csv\": INPUT_CSV,\n",
    "        \"n_rows\": int(df.shape[0]),\n",
    "        \"n_columns_original\": int(df.shape[1]),\n",
    "        \"numeric_cols\": detect_columns(train_df)[0],\n",
    "        \"categorical_cols\": detect_columns(train_df)[1],\n",
    "        \"id_cols\": id_cols,\n",
    "        \"input_dim_after_transform\": int(input_dim),\n",
    "        \"latent_dim\": int(LATENT_DIM),\n",
    "        \"hidden_units_encoder\": HIDDEN_UNITS_ENCODER,\n",
    "        \"mlp_hidden_layers\": hidden_layers\n",
    "    }\n",
    "    with open(os.path.join(OUTPUT_DIR, \"metadata.json\"), \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(meta, fh, indent=2)\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5df112-1f7a-472d-a8da-4e895a23aff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
